{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedLib: Simulating Multi-task Federated Learning using FedLib virtual Federated environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing supportive libaries\n",
    "This notebook shows a demo on PyTorch back-end model impelementation.\n",
    "\n",
    "In the very begining, we import the supporting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from fedlib.utils import get_logger\n",
    "from fedlib.ve.mtfl import MTFLEnv\n",
    "from fedlib.lib import Server, Client\n",
    "from fedlib.networks import resnet20\n",
    "from fedlib.lib.sampler import random_sampler\n",
    "from fedlib.lib.algo.torch.mtfl import Trainer\n",
    "from fedlib.datasets import partition_data, get_dataloader,get_client_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define arguments\n",
    "Here we define arguments. To show an intuitive example, we show the demo store all the parameters in a dictionary in the following code block.\n",
    "We also provide APIs for you create your arguments in a `*.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "args = {}\n",
    "args[\"n_clients\"] = 10\n",
    "args[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args['sample_fn'] = random_sampler\n",
    "args['trainer'] = Trainer(logger)\n",
    "args['communicator'] = None\n",
    "args[\"test_dataset\"] = None\n",
    "args[\"partition\"] = \"noniid-labeldir\"\n",
    "args[\"dataset\"] = \"mnist\"\n",
    "args[\"datadir\"] = \"./data\"\n",
    "args[\"beta\"] = 0.5\n",
    "args[\"batch_size\"] = 64\n",
    "args[\"lr\"] = 0.01\n",
    "args[\"optimizer\"] = \"SGD\"\n",
    "args[\"lr_scheduler\"] = \"ExponentialLR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test dataset for server, and passing it as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data statistics: {0: {0: 1511, 1: 1125, 2: 1939, 3: 159, 4: 1941}, 1: {0: 1378, 1: 1366, 2: 737, 3: 2768}, 2: {0: 267, 1: 378, 2: 2572, 3: 307, 4: 588, 5: 20, 6: 50, 7: 780, 8: 44, 9: 226}, 3: {0: 206, 1: 130, 2: 216, 3: 527, 4: 1110, 5: 1342, 6: 36, 7: 2066, 8: 317, 9: 55}, 4: {0: 176, 1: 759, 2: 107, 3: 48, 4: 89, 5: 726, 6: 770, 7: 496, 8: 1510, 9: 177}, 5: {0: 876, 2: 87, 3: 518, 4: 1295, 5: 52, 6: 12, 7: 737, 8: 2233, 9: 2996}, 6: {0: 233, 1: 688, 2: 39, 3: 549, 4: 158, 5: 318, 6: 593, 7: 680, 8: 365, 9: 1812}, 7: {0: 476, 1: 175, 3: 2, 5: 690, 6: 949, 7: 433, 8: 992, 9: 633}, 8: {0: 405, 1: 279, 2: 253, 3: 63, 4: 248, 5: 1397, 6: 1642, 7: 1073, 8: 389, 9: 50}, 9: {0: 395, 1: 1842, 2: 8, 3: 1190, 4: 413, 5: 876, 6: 1866, 8: 1}}\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, net_dataidx_map, traindata_cls_counts = partition_data(\n",
    "    args[\"dataset\"], args[\"datadir\"], args['partition'], args['n_clients'], beta=args['beta'])\n",
    "n_classes = len(np.unique(y_train))\n",
    "train_dl_global, test_dl_global, train_ds_global, test_ds_global = get_dataloader(args[\"dataset\"],\n",
    "                                                                                    args[\"datadir\"],\n",
    "                                                                                      args[\"batch_size\"],\n",
    "                                                                                      32)\n",
    "args[\"test_dataset\"] = test_dl_global"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Arc\n",
    "Model must contains encoder, decoder, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.predictor = nn.Linear(in_features=32, out_features=10, bias=True)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.predictor(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28]) torch.Size([10, 1, 28, 28]) torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder()\n",
    "x = torch.rand([10,1,28,28])\n",
    "representation = model.encoder(x)\n",
    "x_ = model.decoder(representation)\n",
    "pred = model(x)\n",
    "print(x.shape,x_.shape,pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create server and clients objects\n",
    "Here we use the arguments we defined before, and create server and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args[\"global_model\"] = model.encoder\n",
    "server = Server(**args)\n",
    "clients = {}\n",
    "\n",
    "data_loaders = get_client_dataloader(args[\"dataset\"], args[\"datadir\"], args['batch_size'], 32, net_dataidx_map)\n",
    "\n",
    "criterion_pred = torch.nn.CrossEntropyLoss()\n",
    "criterion_rep = torch.nn.MSELoss()\n",
    "\n",
    "args[\"criterion\"]={\n",
    "    \"criterion_rep\": criterion_rep,\n",
    "    \"criterion_pred\": criterion_pred\n",
    "    }\n",
    "\n",
    "for id in range(args[\"n_clients\"]):\n",
    "    # dataidxs = net_dataidx_map[id]\n",
    "    args[\"id\"] = id\n",
    "    # args[\"trainloader\"], _, _, _ = get_dataloader(args[\"dataset\"], args[\"datadir\"], args['batch_size'], 32, dataidxs)\n",
    "    args[\"trainloader\"] = data_loaders[id]\n",
    "    args[\"model\"] = copy.deepcopy(model)\n",
    "    clients[id] = Client(**args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simulator\n",
    "\n",
    "Simulator simulates the virtual federated learning environments, and run server and clients on single device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = MTFLEnv(server=server, clients=clients, communication_rounds=10,n_clients= 10,sample_rate=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulator\n",
    "User API Simulator.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*******starting rounds 1 optimization******\n",
      "INFO:root:optimize the 4-th clients\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.458877\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.433209\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.365509\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.318931\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.289380\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.270249\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.171733\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.149680\n",
      "INFO:root:Epoch: 0\tLoss: 2.291434\n",
      "INFO:root:Update Epoch: 1 \tLoss: 2.060631\n",
      "INFO:root:Update Epoch: 1 \tLoss: 2.088791\n",
      "INFO:root:Update Epoch: 1 \tLoss: 2.019820\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.965572\n",
      "INFO:root:Update Epoch: 1 \tLoss: 2.101911\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.849362\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.978142\n",
      "INFO:root:Update Epoch: 1 \tLoss: 2.040374\n",
      "INFO:root:Epoch: 1\tLoss: 2.177208\n",
      "INFO:root:*******starting rounds 2 optimization******\n",
      "INFO:root:optimize the 9-th clients\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.450994\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.417016\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.226083\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.022510\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.934955\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.940995\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.891780\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.758760\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.676401\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.645063\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.635265\n",
      "INFO:root:Epoch: 0\tLoss: 1.950511\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.688900\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.783828\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.545300\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.492899\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.508864\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.429320\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.251340\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.202171\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.410848\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.399988\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.419558\n",
      "INFO:root:Epoch: 1\tLoss: 1.702931\n",
      "INFO:root:*******starting rounds 3 optimization******\n",
      "INFO:root:optimize the 7-th clients\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.533422\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.087558\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.971295\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.806134\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.790350\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.866605\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.827555\n",
      "INFO:root:Epoch: 0\tLoss: 1.886070\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.729044\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.752618\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.671006\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.714985\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.641868\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.565182\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.506176\n",
      "INFO:root:Epoch: 1\tLoss: 1.781099\n",
      "INFO:root:*******starting rounds 4 optimization******\n",
      "INFO:root:optimize the 2-th clients\n",
      "INFO:root:Update Epoch: 0 \tLoss: 4.388025\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.633156\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.371513\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.749988\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.619367\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.799425\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.500324\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.485864\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.260129\n",
      "INFO:root:Epoch: 0\tLoss: 1.653746\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.415854\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.324863\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.557026\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.116260\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.393525\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.348491\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.192700\n",
      "INFO:root:Update Epoch: 1 \tLoss: 0.980658\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.036467\n",
      "INFO:root:Epoch: 1\tLoss: 1.471274\n",
      "INFO:root:*******starting rounds 5 optimization******\n",
      "INFO:root:optimize the 6-th clients\n",
      "INFO:root:Update Epoch: 0 \tLoss: 2.330082\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.840698\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.760976\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.802776\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.771365\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.641486\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.708969\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.480177\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.531934\n",
      "INFO:root:Epoch: 0\tLoss: 1.698393\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.557697\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.340108\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.484307\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.410854\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.355038\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.445450\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.478119\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.426449\n",
      "INFO:root:Update Epoch: 1 \tLoss: 1.642862\n",
      "INFO:root:Epoch: 1\tLoss: 1.557888\n",
      "INFO:root:*******starting rounds 6 optimization******\n",
      "INFO:root:optimize the 6-th clients\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.277417\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.578012\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.052863\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.374658\n",
      "INFO:root:Update Epoch: 0 \tLoss: 1.395160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m simulator\u001b[39m.\u001b[39;49mrun(local_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fedlib/lib/python3.10/site-packages/fedlib/ve/mtfl.py:56\u001b[0m, in \u001b[0;36mMTFLEnv.run\u001b[0;34m(self, local_epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mid not match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m client\u001b[39m.\u001b[39mset_model_params(globa_encoder, module_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m client\u001b[39m.\u001b[39;49mclient_update( epochs\u001b[39m=\u001b[39;49mlocal_epochs)\n\u001b[1;32m     58\u001b[0m nets_encoders\u001b[39m.\u001b[39mappend(client\u001b[39m.\u001b[39mget_model_params(module_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     59\u001b[0m local_datasize\u001b[39m.\u001b[39mappend(client\u001b[39m.\u001b[39mdatasize)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fedlib/lib/python3.10/site-packages/fedlib/lib/client.py:66\u001b[0m, in \u001b[0;36mClient.client_update\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\n\u001b[1;32m     64\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion\n\u001b[0;32m---> 66\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fedlib/lib/python3.10/site-packages/fedlib/lib/algo/torch/mtfl/mtfl.py:46\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, model, dataloader, criterion, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m loss \u001b[39m=\u001b[39m criterion_pred(pred_out, labels)\n\u001b[1;32m     44\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion_rep(decodes_out, x)\n\u001b[0;32m---> 46\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     48\u001b[0m \u001b[39m# to avoid nan loss\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m# torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fedlib/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fedlib/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simulator.run(local_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): LambdaLayer()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): LambdaLayer()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010016\n",
      "0.010016\n",
      "0.010617\n",
      "0.019431\n",
      "0.023638\n",
      "0.034355\n",
      "0.034555\n",
      "0.03756\n",
      "0.041667\n",
      "0.046274\n",
      "0.043069\n",
      "0.055889\n",
      "0.061899\n",
      "0.063802\n",
      "0.06891\n",
      "0.075921\n",
      "0.075521\n",
      "0.073117\n",
      "0.078025\n",
      "0.08774\n",
      "0.082332\n",
      "0.086839\n",
      "0.084836\n",
      "0.086438\n",
      "0.086038\n",
      "0.103265\n",
      "0.102664\n",
      "0.102865\n",
      "0.096454\n",
      "0.098357\n",
      "0.106771\n",
      "0.108273\n",
      "0.107472\n",
      "0.108173\n",
      "0.110477\n",
      "0.116086\n",
      "0.121695\n",
      "0.11859\n",
      "0.121494\n",
      "0.121394\n",
      "0.1247\n",
      "0.1248\n",
      "0.128105\n",
      "0.127103\n",
      "0.133514\n",
      "0.125701\n",
      "0.135317\n",
      "0.130609\n",
      "0.133313\n",
      "0.144631\n",
      "0.144431\n",
      "0.140224\n",
      "0.14353\n",
      "0.138822\n",
      "0.141126\n",
      "0.144732\n",
      "0.150942\n",
      "0.150841\n",
      "0.151342\n",
      "0.157151\n",
      "0.159655\n",
      "0.155849\n",
      "0.158454\n",
      "0.155349\n",
      "0.158353\n",
      "0.165465\n",
      "0.161759\n",
      "0.166266\n",
      "0.16246\n",
      "0.165665\n",
      "0.166066\n",
      "0.16857\n",
      "0.164363\n",
      "0.161558\n",
      "0.170172\n",
      "0.170873\n",
      "0.17498\n",
      "0.173478\n",
      "0.170873\n",
      "0.173277\n",
      "0.182692\n",
      "0.179688\n",
      "0.176182\n",
      "0.174379\n",
      "0.178686\n",
      "0.180889\n",
      "0.179487\n",
      "0.17478\n",
      "0.184595\n",
      "0.178285\n",
      "0.18109\n",
      "0.182692\n",
      "0.179988\n",
      "0.186098\n",
      "0.184696\n",
      "0.179587\n",
      "0.185196\n",
      "0.189002\n",
      "0.183293\n",
      "0.188101\n",
      "0.189002\n",
      "0.182091\n",
      "0.185196\n",
      "0.1874\n",
      "0.184595\n",
      "0.196414\n",
      "0.189103\n",
      "0.191707\n",
      "0.19391\n",
      "0.192808\n",
      "0.195613\n",
      "0.19361\n",
      "0.183594\n",
      "0.195813\n",
      "0.198217\n",
      "0.195713\n",
      "0.197416\n",
      "0.193309\n",
      "0.203726\n",
      "0.198518\n",
      "0.201422\n",
      "0.207632\n",
      "0.201322\n",
      "0.196815\n",
      "0.201522\n",
      "0.202123\n",
      "0.203425\n",
      "0.20603\n",
      "0.203225\n",
      "0.205529\n",
      "0.20653\n",
      "0.208033\n",
      "0.206731\n",
      "0.204127\n",
      "0.206731\n",
      "0.205529\n",
      "0.209034\n",
      "0.207732\n",
      "0.207732\n",
      "0.210938\n",
      "0.210537\n",
      "0.209936\n",
      "0.210437\n",
      "0.213942\n",
      "0.211438\n",
      "0.21244\n",
      "0.21244\n",
      "0.216346\n",
      "0.215645\n",
      "0.215745\n",
      "0.211739\n",
      "0.212941\n",
      "0.213742\n",
      "0.213842\n",
      "0.208934\n",
      "0.212139\n",
      "0.210136\n",
      "0.214543\n",
      "0.219451\n",
      "0.217147\n",
      "0.209836\n",
      "0.215244\n",
      "0.214944\n",
      "0.218249\n",
      "0.217648\n",
      "0.218249\n",
      "0.220152\n",
      "0.21865\n",
      "0.217748\n",
      "0.217648\n",
      "0.210437\n",
      "0.217548\n",
      "0.21905\n",
      "0.222957\n",
      "0.22496\n",
      "0.217448\n",
      "0.221454\n",
      "0.216747\n",
      "0.222957\n",
      "0.220553\n",
      "0.22506\n",
      "0.223858\n",
      "0.223458\n",
      "0.216647\n",
      "0.222556\n",
      "0.217248\n",
      "0.224259\n",
      "0.221554\n",
      "0.221254\n",
      "0.220052\n",
      "0.223157\n",
      "0.220453\n",
      "0.224659\n",
      "0.22496\n",
      "0.21905\n",
      "0.215745\n",
      "0.216446\n",
      "0.223357\n",
      "0.223458\n",
      "0.220853\n",
      "0.224159\n",
      "0.221354\n",
      "0.223958\n",
      "0.224058\n",
      "0.217949\n",
      "0.221154\n",
      "0.223958\n",
      "0.216947\n",
      "0.222456\n",
      "0.228165\n",
      "0.230369\n",
      "0.223357\n",
      "0.21895\n",
      "0.221354\n",
      "0.225761\n",
      "0.221655\n",
      "0.225461\n",
      "0.226963\n",
      "0.229167\n",
      "0.227764\n",
      "0.225861\n",
      "0.230268\n",
      "0.222857\n",
      "0.224058\n",
      "0.224459\n",
      "0.231571\n",
      "0.229267\n",
      "0.226863\n",
      "0.232071\n",
      "0.227865\n",
      "0.228065\n",
      "0.224259\n",
      "0.229968\n",
      "0.228466\n",
      "0.230569\n",
      "0.231671\n",
      "0.235276\n",
      "0.228866\n",
      "0.230369\n",
      "0.229768\n",
      "0.227664\n",
      "0.225761\n",
      "0.227264\n",
      "0.235877\n",
      "0.233073\n",
      "0.231771\n",
      "0.231671\n",
      "0.236478\n",
      "0.229067\n",
      "0.224259\n",
      "0.230469\n",
      "0.229768\n",
      "0.228466\n",
      "0.23758\n",
      "0.238081\n",
      "0.235276\n",
      "0.239483\n",
      "0.232372\n",
      "0.234375\n",
      "0.236278\n",
      "0.229267\n",
      "0.234375\n",
      "0.233674\n",
      "0.238081\n",
      "0.233974\n",
      "0.227965\n",
      "0.236078\n",
      "0.23738\n",
      "0.233474\n",
      "0.239083\n",
      "0.235777\n",
      "0.237179\n",
      "0.239083\n",
      "0.236478\n",
      "0.234675\n",
      "0.236078\n",
      "0.236679\n",
      "0.235477\n",
      "0.240585\n",
      "0.232772\n",
      "0.23758\n",
      "0.234976\n",
      "0.23758\n",
      "0.234776\n",
      "0.238482\n",
      "0.236979\n",
      "0.244191\n",
      "0.238682\n",
      "0.234375\n",
      "0.242588\n",
      "0.242688\n",
      "0.237079\n",
      "0.240284\n",
      "0.245393\n",
      "0.240084\n",
      "0.24369\n",
      "0.23147\n",
      "0.244591\n",
      "0.241787\n",
      "0.242288\n",
      "0.238281\n",
      "0.241486\n",
      "0.235877\n",
      "0.23748\n",
      "0.238081\n",
      "0.238582\n",
      "0.241486\n",
      "0.242188\n",
      "0.240385\n",
      "0.235978\n",
      "0.242688\n",
      "0.241486\n",
      "0.238782\n",
      "0.238782\n",
      "0.238081\n",
      "0.239083\n",
      "0.242188\n",
      "0.241086\n",
      "0.241587\n",
      "0.243089\n",
      "0.247596\n",
      "0.245593\n",
      "0.242989\n",
      "0.239083\n",
      "0.242588\n",
      "0.239483\n",
      "0.24379\n",
      "0.241186\n",
      "0.248297\n",
      "0.246595\n",
      "0.244992\n",
      "0.242388\n",
      "0.244191\n",
      "0.24359\n",
      "0.24359\n",
      "0.250901\n",
      "0.250501\n",
      "0.244591\n",
      "0.248898\n",
      "0.248498\n",
      "0.246695\n",
      "0.246394\n",
      "0.246194\n",
      "0.246695\n",
      "0.24349\n",
      "0.242388\n",
      "0.242388\n",
      "0.244391\n",
      "0.244191\n",
      "0.247296\n",
      "0.246294\n",
      "0.250401\n",
      "0.239784\n",
      "0.249099\n",
      "0.246595\n",
      "0.252103\n",
      "0.245192\n",
      "0.250401\n",
      "0.251402\n",
      "0.248698\n",
      "0.246094\n",
      "0.248898\n",
      "0.247296\n",
      "0.244892\n",
      "0.244391\n",
      "0.246494\n",
      "0.253205\n",
      "0.245893\n",
      "0.248598\n",
      "0.245893\n",
      "0.244792\n",
      "0.243089\n",
      "0.253606\n",
      "0.250801\n",
      "0.252905\n",
      "0.246895\n",
      "0.245493\n",
      "0.252003\n",
      "0.250701\n",
      "0.251803\n",
      "0.252804\n",
      "0.253706\n",
      "0.253906\n",
      "0.249399\n",
      "0.244291\n",
      "0.246194\n",
      "0.246695\n",
      "0.248998\n",
      "0.252204\n",
      "0.25611\n",
      "0.256611\n",
      "0.253405\n",
      "0.250901\n",
      "0.251803\n",
      "0.251202\n",
      "0.250501\n",
      "0.2501\n",
      "0.25641\n",
      "0.2499\n",
      "0.255509\n",
      "0.245292\n",
      "0.25641\n",
      "0.252905\n",
      "0.255008\n",
      "0.254808\n",
      "0.252604\n",
      "0.251502\n",
      "0.253906\n",
      "0.254808\n",
      "0.255809\n",
      "0.253906\n",
      "0.253105\n",
      "0.246595\n",
      "0.251803\n",
      "0.251603\n",
      "0.252204\n",
      "0.254107\n",
      "0.257712\n",
      "0.253305\n",
      "0.260116\n",
      "0.258714\n",
      "0.251903\n",
      "0.252604\n",
      "0.254607\n",
      "0.258814\n",
      "0.255208\n",
      "0.256711\n",
      "0.260317\n",
      "0.255809\n",
      "0.252504\n",
      "0.259014\n",
      "0.259916\n",
      "0.254607\n",
      "0.256711\n",
      "0.254507\n",
      "0.251903\n",
      "0.245893\n",
      "0.261518\n",
      "0.25621\n",
      "0.25621\n",
      "0.256611\n",
      "0.255308\n",
      "0.251002\n",
      "0.259215\n",
      "0.258213\n",
      "0.254207\n",
      "0.252404\n",
      "0.256811\n",
      "0.259315\n",
      "0.257011\n",
      "0.257712\n",
      "0.259215\n",
      "0.257512\n",
      "0.252804\n",
      "0.260317\n",
      "0.258714\n",
      "0.259716\n",
      "0.261518\n",
      "0.263522\n",
      "0.258514\n",
      "0.255909\n",
      "0.25621\n",
      "0.25651\n",
      "0.261318\n",
      "0.260417\n",
      "0.252704\n",
      "0.25631\n",
      "0.260417\n",
      "0.258113\n",
      "0.257512\n",
      "0.253706\n",
      "0.250601\n",
      "0.258013\n",
      "0.259215\n",
      "0.259315\n",
      "0.253405\n",
      "0.260016\n",
      "0.257412\n",
      "0.254307\n",
      "0.257412\n",
      "0.258814\n",
      "0.25611\n",
      "0.267628\n",
      "0.26262\n",
      "0.257512\n",
      "0.25611\n",
      "0.258313\n",
      "0.255709\n",
      "0.260016\n",
      "0.257913\n",
      "0.261619\n",
      "0.266526\n",
      "0.263221\n",
      "0.263722\n",
      "0.26222\n",
      "0.263522\n",
      "0.260016\n",
      "0.258914\n",
      "0.264924\n",
      "0.261018\n"
     ]
    }
   ],
   "source": [
    "a =[(1,0.010016),(2,0.010016),(3,0.010617),(4,0.019431),(5,0.023638),(6,0.034355),(7,0.034555),(8,0.037560),(9,0.041667),(10,0.046274),(11,0.043069),(12,0.055889),(13,0.061899),(14,0.063802),(15,0.068910),(16,0.075921),(17,0.075521),(18,0.073117),(19,0.078025),(20,0.087740),(21,0.082332),(22,0.086839),(23,0.084836),(24,0.086438),(25,0.086038),(26,0.103265),(27,0.102664),(28,0.102865),(29,0.096454),(30,0.098357),(31,0.106771),(32,0.108273),(33,0.107472),(34,0.108173),(35,0.110477),(36,0.116086),(37,0.121695),(38,0.118590),(39,0.121494),(40,0.121394),(41,0.124700),(42,0.124800),(43,0.128105),(44,0.127103),(45,0.133514),(46,0.125701),(47,0.135317),(48,0.130609),(49,0.133313),(50,0.144631),(51,0.144431),(52,0.140224),(53,0.143530),(54,0.138822),(55,0.141126),(56,0.144732),(57,0.150942),(58,0.150841),(59,0.151342),(60,0.157151),(61,0.159655),(62,0.155849),(63,0.158454),(64,0.155349),(65,0.158353),(66,0.165465),(67,0.161759),(68,0.166266),(69,0.162460),(70,0.165665),(71,0.166066),(72,0.168570),(73,0.164363),(74,0.161558),(75,0.170172),(76,0.170873),(77,0.174980),(78,0.173478),(79,0.170873),(80,0.173277),(81,0.182692),(82,0.179688),(83,0.176182),(84,0.174379),(85,0.178686),(86,0.180889),(87,0.179487),(88,0.174780),(89,0.184595),(90,0.178285),(91,0.181090),(92,0.182692),(93,0.179988),(94,0.186098),(95,0.184696),(96,0.179587),(97,0.185196),(98,0.189002),(99,0.183293),(100,0.188101),(101,0.189002),(102,0.182091),(103,0.185196),(104,0.187400),(105,0.184595),(106,0.196414),(107,0.189103),(108,0.191707),(109,0.193910),(110,0.192808),(111,0.195613),(112,0.193610),(113,0.183594),(114,0.195813),(115,0.198217),(116,0.195713),(117,0.197416),(118,0.193309),(119,0.203726),(120,0.198518),(121,0.201422),(122,0.207632),(123,0.201322),(124,0.196815),(125,0.201522),(126,0.202123),(127,0.203425),(128,0.206030),(129,0.203225),(130,0.205529),(131,0.206530),(132,0.208033),(133,0.206731),(134,0.204127),(135,0.206731),(136,0.205529),(137,0.209034),(138,0.207732),(139,0.207732),(140,0.210938),(141,0.210537),(142,0.209936),(143,0.210437),(144,0.213942),(145,0.211438),(146,0.212440),(147,0.212440),(148,0.216346),(149,0.215645),(150,0.215745),(151,0.211739),(152,0.212941),(153,0.213742),(154,0.213842),(155,0.208934),(156,0.212139),(157,0.210136),(158,0.214543),(159,0.219451),(160,0.217147),(161,0.209836),(162,0.215244),(163,0.214944),(164,0.218249),(165,0.217648),(166,0.218249),(167,0.220152),(168,0.218650),(169,0.217748),(170,0.217648),(171,0.210437),(172,0.217548),(173,0.219050),(174,0.222957),(175,0.224960),(176,0.217448),(177,0.221454),(178,0.216747),(179,0.222957),(180,0.220553),(181,0.225060),(182,0.223858),(183,0.223458),(184,0.216647),(185,0.222556),(186,0.217248),(187,0.224259),(188,0.221554),(189,0.221254),(190,0.220052),(191,0.223157),(192,0.220453),(193,0.224659),(194,0.224960),(195,0.219050),(196,0.215745),(197,0.216446),(198,0.223357),(199,0.223458),(200,0.220853),(201,0.224159),(202,0.221354),(203,0.223958),(204,0.224058),(205,0.217949),(206,0.221154),(207,0.223958),(208,0.216947),(209,0.222456),(210,0.228165),(211,0.230369),(212,0.223357),(213,0.218950),(214,0.221354),(215,0.225761),(216,0.221655),(217,0.225461),(218,0.226963),(219,0.229167),(220,0.227764),(221,0.225861),(222,0.230268),(223,0.222857),(224,0.224058),(225,0.224459),(226,0.231571),(227,0.229267),(228,0.226863),(229,0.232071),(230,0.227865),(231,0.228065),(232,0.224259),(233,0.229968),(234,0.228466),(235,0.230569),(236,0.231671),(237,0.235276),(238,0.228866),(239,0.230369),(240,0.229768),(241,0.227664),(242,0.225761),(243,0.227264),(244,0.235877),(245,0.233073),(246,0.231771),(247,0.231671),(248,0.236478),(249,0.229067),(250,0.224259),(251,0.230469),(252,0.229768),(253,0.228466),(254,0.237580),(255,0.238081),(256,0.235276),(257,0.239483),(258,0.232372),(259,0.234375),(260,0.236278),(261,0.229267),(262,0.234375),(263,0.233674),(264,0.238081),(265,0.233974),(266,0.227965),(267,0.236078),(268,0.237380),(269,0.233474),(270,0.239083),(271,0.235777),(272,0.237179),(273,0.239083),(274,0.236478),(275,0.234675),(276,0.236078),(277,0.236679),(278,0.235477),(279,0.240585),(280,0.232772),(281,0.237580),(282,0.234976),(283,0.237580),(284,0.234776),(285,0.238482),(286,0.236979),(287,0.244191),(288,0.238682),(289,0.234375),(290,0.242588),(291,0.242688),(292,0.237079),(293,0.240284),(294,0.245393),(295,0.240084),(296,0.243690),(297,0.231470),(298,0.244591),(299,0.241787),(300,0.242288),(301,0.238281),(302,0.241486),(303,0.235877),(304,0.237480),(305,0.238081),(306,0.238582),(307,0.241486),(308,0.242188),(309,0.240385),(310,0.235978),(311,0.242688),(312,0.241486),(313,0.238782),(314,0.238782),(315,0.238081),(316,0.239083),(317,0.242188),(318,0.241086),(319,0.241587),(320,0.243089),(321,0.247596),(322,0.245593),(323,0.242989),(324,0.239083),(325,0.242588),(326,0.239483),(327,0.243790),(328,0.241186),(329,0.248297),(330,0.246595),(331,0.244992),(332,0.242388),(333,0.244191),(334,0.243590),(335,0.243590),(336,0.250901),(337,0.250501),(338,0.244591),(339,0.248898),(340,0.248498),(341,0.246695),(342,0.246394),(343,0.246194),(344,0.246695),(345,0.243490),(346,0.242388),(347,0.242388),(348,0.244391),(349,0.244191),(350,0.247296),(351,0.246294),(352,0.250401),(353,0.239784),(354,0.249099),(355,0.246595),(356,0.252103),(357,0.245192),(358,0.250401),(359,0.251402),(360,0.248698),(361,0.246094),(362,0.248898),(363,0.247296),(364,0.244892),(365,0.244391),(366,0.246494),(367,0.253205),(368,0.245893),(369,0.248598),(370,0.245893),(371,0.244792),(372,0.243089),(373,0.253606),(374,0.250801),(375,0.252905),(376,0.246895),(377,0.245493),(378,0.252003),(379,0.250701),(380,0.251803),(381,0.252804),(382,0.253706),(383,0.253906),(384,0.249399),(385,0.244291),(386,0.246194),(387,0.246695),(388,0.248998),(389,0.252204),(390,0.256110),(391,0.256611),(392,0.253405),(393,0.250901),(394,0.251803),(395,0.251202),(396,0.250501),(397,0.250100),(398,0.256410),(399,0.249900),(400,0.255509),(401,0.245292),(402,0.256410),(403,0.252905),(404,0.255008),(405,0.254808),(406,0.252604),(407,0.251502),(408,0.253906),(409,0.254808),(410,0.255809),(411,0.253906),(412,0.253105),(413,0.246595),(414,0.251803),(415,0.251603),(416,0.252204),(417,0.254107),(418,0.257712),(419,0.253305),(420,0.260116),(421,0.258714),(422,0.251903),(423,0.252604),(424,0.254607),(425,0.258814),(426,0.255208),(427,0.256711),(428,0.260317),(429,0.255809),(430,0.252504),(431,0.259014),(432,0.259916),(433,0.254607),(434,0.256711),(435,0.254507),(436,0.251903),(437,0.245893),(438,0.261518),(439,0.256210),(440,0.256210),(441,0.256611),(442,0.255308),(443,0.251002),(444,0.259215),(445,0.258213),(446,0.254207),(447,0.252404),(448,0.256811),(449,0.259315),(450,0.257011),(451,0.257712),(452,0.259215),(453,0.257512),(454,0.252804),(455,0.260317),(456,0.258714),(457,0.259716),(458,0.261518),(459,0.263522),(460,0.258514),(461,0.255909),(462,0.256210),(463,0.256510),(464,0.261318),(465,0.260417),(466,0.252704),(467,0.256310),(468,0.260417),(469,0.258113),(470,0.257512),(471,0.253706),(472,0.250601),(473,0.258013),(474,0.259215),(475,0.259315),(476,0.253405),(477,0.260016),(478,0.257412),(479,0.254307),(480,0.257412),(481,0.258814),(482,0.256110),(483,0.267628),(484,0.262620),(485,0.257512),(486,0.256110),(487,0.258313),(488,0.255709),(489,0.260016),(490,0.257913),(491,0.261619),(492,0.266526),(493,0.263221),(494,0.263722),(495,0.262220),(496,0.263522),(497,0.260016),(498,0.258914),(499,0.264924),(500,0.261018)]\n",
    "\n",
    "\n",
    "\n",
    "for (i,k) in a:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.950368240000007"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "l = [12.133472, 11.051832, 14.131328, 14.266, 13.902304, 13.031408, 12.846048, 14.419544, 14.98216, 14.098432, 13.224656, 13.642752, 12.103536, 13.72976, 14.13528, 12.516888, 14.698832, 13.97496, 14.78284, 15.706896, 15.643472, 14.685232, 12.855568, 12.993528, 15.17564, 12.004216, 12.179992, 15.179248, 13.713744, 16.09056, 15.052384, 13.680304, 14.986608, 13.562448, 16.899536, 14.891776, 13.510264, 15.33912, 15.10424, 13.361208, 12.860784, 13.34252, 15.169296, 14.427352, 12.432376, 14.166144, 14.353368, 13.100632, 13.174512, 14.316856, 13.071, 12.27112, 12.698736, 15.270704, 11.845976, 12.505952, 11.827648, 16.17976, 15.293376, 13.908896, 14.55068, 12.569904, 14.696304, 14.809072, 13.670112, 14.40056, 14.148088, 14.937336, 14.249408, 12.347672, 13.689808, 15.529656, 13.90156, 14.52136, 15.370016, 12.95068, 13.760152, 13.640736, 13.384896, 13.940432, 12.288096, 16.022944, 13.67304, 16.3048, 14.555064, 15.112504, 13.870744, 14.602264, 13.578128, 15.910176, 13.262008, 13.884064, 14.88256, 13.72896, 12.365976, 12.424472, 11.237752, 13.702424, 15.667224, 14.390168]\n",
    "sum(l)/len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TinyImageNetDataset(Dataset):\n",
    "  def __init__(self, root_dir, mode='train', preload=True, load_transform=None,\n",
    "               transform=None, download=False, max_samples=None):\n",
    "    tinp = TinyImageNetPaths(root_dir, download)\n",
    "    self.mode = mode\n",
    "    self.label_idx = 1  # from [image, id, nid, box]\n",
    "    self.preload = preload\n",
    "    self.transform = transform\n",
    "    self.transform_results = dict()\n",
    "\n",
    "    self.IMAGE_SHAPE = (64, 64, 3)\n",
    "\n",
    "    self.img_data = []\n",
    "    self.label_data = []\n",
    "\n",
    "    self.max_samples = max_samples\n",
    "    self.samples = tinp.paths[mode]\n",
    "    self.samples_num = len(self.samples)\n",
    "\n",
    "    if self.max_samples is not None:\n",
    "      self.samples_num = min(self.max_samples, self.samples_num)\n",
    "      self.samples = np.random.permutation(self.samples)[:self.samples_num]\n",
    "\n",
    "    if self.preload:\n",
    "      load_desc = \"Preloading {} data...\".format(mode)\n",
    "      self.img_data = np.zeros((self.samples_num,) + self.IMAGE_SHAPE,\n",
    "                               dtype=np.float32)\n",
    "      self.label_data = np.zeros((self.samples_num,), dtype=np.int)\n",
    "      for idx in tqdm(range(self.samples_num), desc=load_desc):\n",
    "        s = self.samples[idx]\n",
    "        img = imageio.imread(s[0])\n",
    "        img = _add_channels(img)\n",
    "        self.img_data[idx] = img\n",
    "        if mode != 'test':\n",
    "          self.label_data[idx] = s[self.label_idx]\n",
    "\n",
    "      if load_transform:\n",
    "        for lt in load_transform:\n",
    "          result = lt(self.img_data, self.label_data)\n",
    "          self.img_data, self.label_data = result[:2]\n",
    "          if len(result) > 2:\n",
    "            self.transform_results.update(result[2])\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.samples_num\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if self.preload:\n",
    "      img = self.img_data[idx]\n",
    "      lbl = None if self.mode == 'test' else self.label_data[idx]\n",
    "    else:\n",
    "      s = self.samples[idx]\n",
    "      img = imageio.imread(s[0])\n",
    "      lbl = None if self.mode == 'test' else s[self.label_idx]\n",
    "    sample = {'image': img, 'label': lbl}\n",
    "\n",
    "    if self.transform:\n",
    "      sample = self.transform(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "train_ds = torchvision.datasets.ImageFolder('data/test',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    transforms.Resize(32), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR10, CIFAR100, SVHN, FashionMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_ds.__len__()\n",
    "train_ds.__getitem__(0)[0].shape\n",
    "dl = torch.utils.data.DataLoader(train_ds,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dl:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "                transforms.ToTensor()])\n",
    "cifar = CIFAR10(\"../src/data\", True, transform_train, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
      "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
      "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
      "         ...,\n",
      "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
      "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
      "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
      "\n",
      "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
      "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
      "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
      "         ...,\n",
      "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
      "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
      "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
      "\n",
      "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
      "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
      "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
      "         ...,\n",
      "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
      "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
      "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])\n",
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(cifar.__getitem__(0)[0])\n",
    "print(cifar.data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_cifar = torch.utils.data.DataLoader(cifar,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dl_cifar:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 9])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4,5,9])\n",
    "idx  = [1,3,5]\n",
    "b = a[idx]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cifar\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(idx)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fedlib/lib/python3.10/site-packages/torchvision/datasets/cifar.py:111\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m        index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets[index]\n\u001b[1;32m    113\u001b[0m     \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import os\n",
    "class TinyImageNet_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        # cifar_dataobj = CIFAR10(self.root, self.train, self.transform, self.target_transform, self.download)\n",
    "        if self.train:\n",
    "            data_dir = os.path.join(self.root,\"train\")\n",
    "        else:\n",
    "            data_dir = os.path.join(self.root,\"val\")\n",
    "        ti_dataobj = torchvision.datasets.ImageFolder(data_dir,\n",
    "                                    transform=self.transform)\n",
    "        \n",
    "        target = ti_dataobj.targets\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.dataidxs is not None:\n",
    "            for i in self.dataidxs:\n",
    "                data.append(ti_dataobj.__getitem__(i)[0])\n",
    "\n",
    "        else:\n",
    "            for i in range(ti_dataobj.__len__()):\n",
    "                data.append(ti_dataobj.__getitem__(i)[0])\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def truncate_channel(self, index):\n",
    "        for i in range(index.shape[0]):\n",
    "            gs_index = index[i]\n",
    "            self.data[gs_index, :, :, 1] = 0.0\n",
    "            self.data[gs_index, :, :, 2] = 0.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "\n",
    "        # if self.transform is not None:\n",
    "        #     img = self.transform(img)\n",
    "\n",
    "        # if self.target_transform is not None:\n",
    "        #     target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_data = TinyImageNet_truncated(\"data\",train=True,dataidxs=[127,236,888], transform=transforms.Compose([\n",
    "                                    transforms.Resize(32), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(ti_data.target)\n",
    "np.where(a == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_ti = torch.utils.data.DataLoader(ti_data,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "train_ds = torchvision.datasets.ImageFolder('data/train',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    transforms.Resize(32), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    ]))\n",
    "dl_ti2 = torch.utils.data.DataLoader(train_ds,batch_size=2,sampler=SubsetRandomSampler([888,236,127]))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "print(len(dl_ti),len(dl_ti2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1333, 0.1294, 0.0784,  ..., 0.4784, 0.3922, 0.3255],\n",
      "          [0.1137, 0.1451, 0.0941,  ..., 0.5529, 0.4941, 0.3647],\n",
      "          [0.1176, 0.1608, 0.1059,  ..., 0.6706, 0.5922, 0.4275],\n",
      "          ...,\n",
      "          [0.0196, 0.0118, 0.0353,  ..., 0.0745, 0.0627, 0.0588],\n",
      "          [0.0078, 0.0275, 0.0627,  ..., 0.0863, 0.0745, 0.0706],\n",
      "          [0.0157, 0.0627, 0.1569,  ..., 0.0902, 0.0706, 0.0627]],\n",
      "\n",
      "         [[0.4392, 0.4431, 0.3804,  ..., 0.6627, 0.5765, 0.5059],\n",
      "          [0.4196, 0.4588, 0.3961,  ..., 0.7176, 0.6745, 0.5569],\n",
      "          [0.4235, 0.4784, 0.4078,  ..., 0.7922, 0.7686, 0.6392],\n",
      "          ...,\n",
      "          [0.2471, 0.2314, 0.2627,  ..., 0.2157, 0.2118, 0.2118],\n",
      "          [0.2275, 0.2431, 0.2784,  ..., 0.2000, 0.2039, 0.2078],\n",
      "          [0.2353, 0.2784, 0.3686,  ..., 0.1882, 0.1882, 0.1922]],\n",
      "\n",
      "         [[0.5686, 0.4941, 0.4627,  ..., 0.7255, 0.6824, 0.6471],\n",
      "          [0.5333, 0.4980, 0.4667,  ..., 0.7725, 0.7647, 0.6706],\n",
      "          [0.5098, 0.4941, 0.4627,  ..., 0.8353, 0.8353, 0.7176],\n",
      "          ...,\n",
      "          [0.2275, 0.1961, 0.2039,  ..., 0.1451, 0.1412, 0.1451],\n",
      "          [0.1686, 0.1725, 0.1882,  ..., 0.1255, 0.1294, 0.1333],\n",
      "          [0.1373, 0.1725, 0.2471,  ..., 0.1137, 0.1137, 0.1176]]],\n",
      "\n",
      "\n",
      "        [[[0.6941, 0.3098, 0.1529,  ..., 0.1020, 0.1529, 0.1804],\n",
      "          [0.6941, 0.3098, 0.1490,  ..., 0.1020, 0.1529, 0.1804],\n",
      "          [0.6980, 0.3098, 0.1451,  ..., 0.1020, 0.1529, 0.1804],\n",
      "          ...,\n",
      "          [0.6039, 0.3059, 0.1961,  ..., 0.1333, 0.1529, 0.2784],\n",
      "          [0.5882, 0.3020, 0.2118,  ..., 0.1333, 0.1529, 0.2784],\n",
      "          [0.5765, 0.3020, 0.2157,  ..., 0.1333, 0.1529, 0.2784]],\n",
      "\n",
      "         [[0.8431, 0.7255, 0.6549,  ..., 0.5725, 0.5490, 0.5098],\n",
      "          [0.8431, 0.7255, 0.6510,  ..., 0.5725, 0.5490, 0.5098],\n",
      "          [0.8431, 0.7255, 0.6471,  ..., 0.5725, 0.5490, 0.5098],\n",
      "          ...,\n",
      "          [0.8627, 0.6980, 0.6392,  ..., 0.5725, 0.5412, 0.5137],\n",
      "          [0.8667, 0.6980, 0.6314,  ..., 0.5725, 0.5412, 0.5137],\n",
      "          [0.8706, 0.6980, 0.6275,  ..., 0.5725, 0.5412, 0.5137]],\n",
      "\n",
      "         [[0.8824, 0.7333, 0.7098,  ..., 0.6431, 0.5804, 0.5098],\n",
      "          [0.8824, 0.7333, 0.7059,  ..., 0.6431, 0.5804, 0.5098],\n",
      "          [0.8824, 0.7333, 0.7020,  ..., 0.6431, 0.5804, 0.5098],\n",
      "          ...,\n",
      "          [0.8549, 0.7882, 0.7843,  ..., 0.6314, 0.5412, 0.5020],\n",
      "          [0.8431, 0.7843, 0.7961,  ..., 0.6314, 0.5412, 0.5020],\n",
      "          [0.8392, 0.7804, 0.8000,  ..., 0.6314, 0.5412, 0.5020]]]])\n",
      "tensor([[[[0.3961, 0.3569, 0.3255,  ..., 0.4392, 0.3843, 0.2980],\n",
      "          [0.2980, 0.2941, 0.3216,  ..., 0.4549, 0.4510, 0.2745],\n",
      "          [0.3412, 0.3490, 0.3059,  ..., 0.4118, 0.4353, 0.4706],\n",
      "          ...,\n",
      "          [0.4235, 0.4510, 0.3451,  ..., 0.3176, 0.3176, 0.3569],\n",
      "          [0.4039, 0.4392, 0.3569,  ..., 0.3490, 0.3216, 0.2784],\n",
      "          [0.4000, 0.4353, 0.4549,  ..., 0.2941, 0.3333, 0.2902]],\n",
      "\n",
      "         [[0.3765, 0.3451, 0.3255,  ..., 0.4549, 0.4000, 0.3098],\n",
      "          [0.3255, 0.3216, 0.3529,  ..., 0.4588, 0.4588, 0.2784],\n",
      "          [0.4118, 0.4196, 0.3686,  ..., 0.4039, 0.4275, 0.4627],\n",
      "          ...,\n",
      "          [0.4157, 0.4431, 0.3333,  ..., 0.2706, 0.2784, 0.3294],\n",
      "          [0.3804, 0.4157, 0.3412,  ..., 0.3137, 0.2941, 0.2667],\n",
      "          [0.3686, 0.4118, 0.4392,  ..., 0.2667, 0.3137, 0.2824]],\n",
      "\n",
      "         [[0.2510, 0.2275, 0.2157,  ..., 0.3294, 0.2941, 0.2314],\n",
      "          [0.1804, 0.1882, 0.2353,  ..., 0.3451, 0.3608, 0.2078],\n",
      "          [0.2471, 0.2667, 0.2392,  ..., 0.3137, 0.3529, 0.4078],\n",
      "          ...,\n",
      "          [0.3216, 0.3490, 0.2353,  ..., 0.2196, 0.2314, 0.2784],\n",
      "          [0.3059, 0.3412, 0.2588,  ..., 0.2824, 0.2588, 0.2275],\n",
      "          [0.3059, 0.3373, 0.3569,  ..., 0.2392, 0.2902, 0.2549]]]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dl_ti:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1333, 0.1294, 0.0784,  ..., 0.4784, 0.3922, 0.3255],\n",
      "          [0.1137, 0.1451, 0.0941,  ..., 0.5529, 0.4941, 0.3647],\n",
      "          [0.1176, 0.1608, 0.1059,  ..., 0.6706, 0.5922, 0.4275],\n",
      "          ...,\n",
      "          [0.0196, 0.0118, 0.0353,  ..., 0.0745, 0.0627, 0.0588],\n",
      "          [0.0078, 0.0275, 0.0627,  ..., 0.0863, 0.0745, 0.0706],\n",
      "          [0.0157, 0.0627, 0.1569,  ..., 0.0902, 0.0706, 0.0627]],\n",
      "\n",
      "         [[0.4392, 0.4431, 0.3804,  ..., 0.6627, 0.5765, 0.5059],\n",
      "          [0.4196, 0.4588, 0.3961,  ..., 0.7176, 0.6745, 0.5569],\n",
      "          [0.4235, 0.4784, 0.4078,  ..., 0.7922, 0.7686, 0.6392],\n",
      "          ...,\n",
      "          [0.2471, 0.2314, 0.2627,  ..., 0.2157, 0.2118, 0.2118],\n",
      "          [0.2275, 0.2431, 0.2784,  ..., 0.2000, 0.2039, 0.2078],\n",
      "          [0.2353, 0.2784, 0.3686,  ..., 0.1882, 0.1882, 0.1922]],\n",
      "\n",
      "         [[0.5686, 0.4941, 0.4627,  ..., 0.7255, 0.6824, 0.6471],\n",
      "          [0.5333, 0.4980, 0.4667,  ..., 0.7725, 0.7647, 0.6706],\n",
      "          [0.5098, 0.4941, 0.4627,  ..., 0.8353, 0.8353, 0.7176],\n",
      "          ...,\n",
      "          [0.2275, 0.1961, 0.2039,  ..., 0.1451, 0.1412, 0.1451],\n",
      "          [0.1686, 0.1725, 0.1882,  ..., 0.1255, 0.1294, 0.1333],\n",
      "          [0.1373, 0.1725, 0.2471,  ..., 0.1137, 0.1137, 0.1176]]],\n",
      "\n",
      "\n",
      "        [[[0.6941, 0.3098, 0.1529,  ..., 0.1020, 0.1529, 0.1804],\n",
      "          [0.6941, 0.3098, 0.1490,  ..., 0.1020, 0.1529, 0.1804],\n",
      "          [0.6980, 0.3098, 0.1451,  ..., 0.1020, 0.1529, 0.1804],\n",
      "          ...,\n",
      "          [0.6039, 0.3059, 0.1961,  ..., 0.1333, 0.1529, 0.2784],\n",
      "          [0.5882, 0.3020, 0.2118,  ..., 0.1333, 0.1529, 0.2784],\n",
      "          [0.5765, 0.3020, 0.2157,  ..., 0.1333, 0.1529, 0.2784]],\n",
      "\n",
      "         [[0.8431, 0.7255, 0.6549,  ..., 0.5725, 0.5490, 0.5098],\n",
      "          [0.8431, 0.7255, 0.6510,  ..., 0.5725, 0.5490, 0.5098],\n",
      "          [0.8431, 0.7255, 0.6471,  ..., 0.5725, 0.5490, 0.5098],\n",
      "          ...,\n",
      "          [0.8627, 0.6980, 0.6392,  ..., 0.5725, 0.5412, 0.5137],\n",
      "          [0.8667, 0.6980, 0.6314,  ..., 0.5725, 0.5412, 0.5137],\n",
      "          [0.8706, 0.6980, 0.6275,  ..., 0.5725, 0.5412, 0.5137]],\n",
      "\n",
      "         [[0.8824, 0.7333, 0.7098,  ..., 0.6431, 0.5804, 0.5098],\n",
      "          [0.8824, 0.7333, 0.7059,  ..., 0.6431, 0.5804, 0.5098],\n",
      "          [0.8824, 0.7333, 0.7020,  ..., 0.6431, 0.5804, 0.5098],\n",
      "          ...,\n",
      "          [0.8549, 0.7882, 0.7843,  ..., 0.6314, 0.5412, 0.5020],\n",
      "          [0.8431, 0.7843, 0.7961,  ..., 0.6314, 0.5412, 0.5020],\n",
      "          [0.8392, 0.7804, 0.8000,  ..., 0.6314, 0.5412, 0.5020]]]])\n",
      "tensor([[[[0.3961, 0.3569, 0.3255,  ..., 0.4392, 0.3843, 0.2980],\n",
      "          [0.2980, 0.2941, 0.3216,  ..., 0.4549, 0.4510, 0.2745],\n",
      "          [0.3412, 0.3490, 0.3059,  ..., 0.4118, 0.4353, 0.4706],\n",
      "          ...,\n",
      "          [0.4235, 0.4510, 0.3451,  ..., 0.3176, 0.3176, 0.3569],\n",
      "          [0.4039, 0.4392, 0.3569,  ..., 0.3490, 0.3216, 0.2784],\n",
      "          [0.4000, 0.4353, 0.4549,  ..., 0.2941, 0.3333, 0.2902]],\n",
      "\n",
      "         [[0.3765, 0.3451, 0.3255,  ..., 0.4549, 0.4000, 0.3098],\n",
      "          [0.3255, 0.3216, 0.3529,  ..., 0.4588, 0.4588, 0.2784],\n",
      "          [0.4118, 0.4196, 0.3686,  ..., 0.4039, 0.4275, 0.4627],\n",
      "          ...,\n",
      "          [0.4157, 0.4431, 0.3333,  ..., 0.2706, 0.2784, 0.3294],\n",
      "          [0.3804, 0.4157, 0.3412,  ..., 0.3137, 0.2941, 0.2667],\n",
      "          [0.3686, 0.4118, 0.4392,  ..., 0.2667, 0.3137, 0.2824]],\n",
      "\n",
      "         [[0.2510, 0.2275, 0.2157,  ..., 0.3294, 0.2941, 0.2314],\n",
      "          [0.1804, 0.1882, 0.2353,  ..., 0.3451, 0.3608, 0.2078],\n",
      "          [0.2471, 0.2667, 0.2392,  ..., 0.3137, 0.3529, 0.4078],\n",
      "          ...,\n",
      "          [0.3216, 0.3490, 0.2353,  ..., 0.2196, 0.2314, 0.2784],\n",
      "          [0.3059, 0.3412, 0.2588,  ..., 0.2824, 0.2588, 0.2275],\n",
      "          [0.3059, 0.3373, 0.3569,  ..., 0.2392, 0.2902, 0.2549]]]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dl_ti2:\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14599062b2f9b2c02bf607b744cd02923df131fc806bc02ca9049e6dcc66a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
