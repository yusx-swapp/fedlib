{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedLib : Heterogeneous Federated Learning using Dynamic Model Pruning and Adaptive Gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing supportive libaries\n",
    "This notebook shows a demo on PyTorch back-end model impelementation.\n",
    "\n",
    "In the very begining, we import the supporting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedlib.networks import resnet20\n",
    "\n",
    "from fedlib.lib.algo import feddp\n",
    "\n",
    "from fedlib.ve import simulator\n",
    "from fedlib.lib import Server\n",
    "import fedlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset \n",
    "Here we initialize Non-IID data for clients and test data for both clients and server.\n",
    "You could use `fedlib.init_dataset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data statistics: {0: {0: 20, 1: 24, 2: 40, 3: 69, 4: 98, 5: 10, 7: 108, 8: 68, 9: 39}, 1: {0: 80, 1: 111, 2: 95, 3: 3, 4: 121, 5: 17, 6: 1, 8: 185}, 2: {0: 3, 1: 84, 2: 14, 3: 69, 4: 4, 5: 51, 6: 18, 7: 1, 8: 145, 9: 28}, 3: {0: 169, 1: 47, 2: 38, 3: 2, 4: 42, 5: 2, 6: 1, 7: 50, 9: 8}, 4: {0: 45, 1: 32, 2: 10, 3: 4, 4: 12, 5: 109, 6: 9, 7: 6, 8: 19, 9: 12}, 5: {0: 138, 1: 4, 2: 15, 3: 25, 4: 15, 5: 112, 6: 1, 7: 8, 8: 49, 9: 33}, 6: {0: 5, 1: 1, 2: 84, 3: 31, 4: 61, 5: 2, 6: 6, 7: 19, 8: 74, 9: 13}, 7: {0: 83, 1: 10, 2: 8, 3: 5, 4: 48, 5: 81, 6: 9, 7: 18, 8: 6, 9: 22}, 8: {0: 1, 1: 1, 2: 41, 3: 39, 4: 74, 5: 18, 6: 83, 7: 1, 8: 63, 9: 38}, 9: {0: 142, 1: 23, 2: 19, 3: 91, 4: 2, 5: 178, 6: 27, 7: 3, 8: 86}, 10: {0: 73, 1: 11, 2: 30, 3: 7, 5: 39, 6: 3, 7: 2, 8: 17, 9: 2}, 11: {0: 8, 2: 93, 3: 37, 4: 12, 5: 19, 6: 7, 7: 128, 8: 13}, 12: {0: 4, 1: 9, 2: 70, 3: 8, 4: 1, 5: 27, 7: 90, 8: 10, 9: 5}, 13: {0: 3, 1: 1, 2: 112, 3: 129, 4: 155, 5: 15, 6: 25, 7: 344}, 14: {0: 131, 1: 45, 2: 85, 3: 15, 4: 5, 5: 7, 6: 1, 8: 146, 9: 1}, 15: {0: 221, 1: 62, 2: 162, 3: 3, 4: 18, 5: 33, 6: 102}, 16: {0: 65, 1: 78, 2: 11, 3: 50, 4: 221, 5: 17, 6: 77}, 17: {0: 5, 1: 7, 2: 360, 3: 39, 4: 18, 5: 142}, 18: {0: 41, 1: 80, 2: 115, 3: 22, 4: 7, 5: 7, 6: 75, 8: 3, 9: 3}, 19: {0: 19, 2: 5, 3: 4, 4: 161, 5: 65, 6: 53, 7: 157, 8: 38}, 20: {0: 71, 1: 11, 3: 14, 4: 43, 5: 6, 7: 19, 8: 63, 9: 95}, 21: {0: 141, 1: 32, 2: 132, 3: 93, 4: 115}, 22: {0: 76, 1: 218, 2: 128, 3: 17, 4: 16, 5: 173}, 23: {1: 116, 2: 46, 3: 115, 4: 210, 5: 139}, 24: {0: 38, 1: 26, 2: 2, 3: 27, 4: 23, 5: 90, 6: 81, 7: 45, 8: 107, 9: 5}, 25: {0: 26, 1: 1, 2: 10, 4: 98, 5: 82, 6: 17, 7: 18, 8: 42, 9: 41}, 26: {1: 3, 2: 98, 3: 207, 4: 90, 5: 213}, 27: {0: 177, 1: 3, 2: 34, 3: 50, 5: 91, 6: 290}, 28: {0: 35, 1: 16, 2: 3, 3: 28, 4: 74, 5: 50, 6: 81, 7: 165, 8: 139}, 29: {0: 19, 2: 28, 4: 21, 5: 13, 7: 6, 8: 227, 9: 164}, 30: {0: 26, 1: 13, 2: 8, 3: 66, 4: 10, 5: 24, 6: 71, 7: 56, 8: 55, 9: 21}, 31: {0: 32, 1: 435, 4: 250}, 32: {0: 3, 1: 51, 2: 11, 3: 163, 4: 10, 5: 105, 6: 42, 7: 6, 8: 9, 9: 131}, 33: {0: 129, 1: 45, 2: 122, 3: 57, 5: 118, 6: 1, 7: 9, 8: 54}, 34: {0: 186, 1: 8, 3: 297, 4: 25}, 35: {0: 26, 1: 12, 2: 37, 3: 236, 6: 36, 7: 6, 8: 3, 9: 3}, 36: {1: 10, 2: 1, 4: 18, 5: 92, 6: 95, 7: 78, 8: 1, 9: 5}, 37: {0: 9, 1: 17, 2: 46, 3: 1, 4: 18, 6: 4, 7: 2, 8: 56, 9: 639}, 38: {0: 5, 1: 86, 2: 134, 4: 15, 5: 15, 6: 39, 7: 88, 8: 1, 9: 156}, 39: {0: 10, 1: 6, 2: 9, 3: 6, 4: 7, 5: 81, 6: 14, 7: 146, 8: 80, 9: 4}, 40: {0: 2, 1: 37, 2: 92, 3: 2, 4: 13, 5: 158, 6: 2, 7: 488}, 41: {0: 31, 1: 242, 2: 385}, 42: {0: 83, 1: 2, 2: 252, 3: 6, 4: 3, 5: 12, 6: 141, 7: 13}, 43: {0: 12, 1: 64, 2: 45, 3: 350, 4: 3, 5: 78}, 44: {0: 217, 1: 22, 2: 76, 3: 19, 4: 27, 5: 28, 6: 314}, 45: {0: 2, 1: 1, 2: 26, 3: 64, 4: 11, 5: 17, 6: 302, 7: 62, 8: 79}, 46: {0: 3, 1: 21, 2: 3, 3: 174, 4: 60, 5: 60, 6: 26, 7: 13, 8: 98, 9: 14}, 47: {0: 118, 1: 10, 2: 64, 3: 2, 4: 27, 6: 5, 7: 29, 8: 1, 9: 4}, 48: {1: 73, 2: 146, 3: 32, 5: 96, 6: 36, 7: 117}, 49: {0: 156, 1: 2, 2: 4, 3: 48, 4: 12, 5: 75, 6: 49, 7: 135, 9: 299}, 50: {0: 2, 1: 30, 2: 11, 3: 5, 4: 354, 5: 28, 6: 1, 7: 288}, 51: {0: 9, 1: 4, 2: 104, 3: 14, 4: 10, 5: 2, 6: 56, 7: 133, 8: 3, 9: 103}, 52: {0: 8, 1: 69, 2: 15, 3: 374, 4: 133}, 53: {0: 16, 1: 1, 2: 10, 3: 5, 4: 26, 5: 46, 6: 38, 7: 201, 8: 35, 9: 47}, 54: {0: 10, 1: 9, 2: 1, 3: 44, 4: 93, 5: 24, 6: 89, 7: 140, 8: 263}, 55: {0: 81, 1: 5, 2: 2, 3: 34, 4: 33, 5: 27, 6: 33, 7: 17, 8: 6, 9: 147}, 56: {0: 8, 1: 15, 2: 1, 3: 34, 4: 135, 5: 62, 6: 18, 7: 1, 8: 18, 9: 167}, 57: {0: 73, 1: 13, 3: 24, 4: 3, 5: 22, 6: 8, 7: 78, 8: 10, 9: 58}, 58: {0: 17, 1: 4, 2: 8, 3: 103, 4: 19, 5: 10, 6: 29, 7: 3, 8: 94, 9: 53}, 59: {0: 8, 1: 24, 2: 12, 3: 6, 4: 322, 5: 38, 6: 269}, 60: {0: 21, 1: 39, 2: 81, 3: 4, 4: 15, 5: 13, 6: 11, 7: 216, 8: 3, 9: 20}, 61: {0: 6, 1: 6, 2: 3, 3: 5, 5: 9, 6: 3, 7: 67, 8: 85}, 62: {0: 30, 1: 202, 2: 33, 3: 59, 4: 64, 6: 63, 8: 11, 9: 23}, 63: {0: 162, 1: 114, 2: 30, 3: 9, 4: 24, 5: 43, 6: 61, 7: 5, 9: 15}, 64: {0: 8, 1: 217, 2: 1, 3: 10, 4: 173, 5: 110}, 65: {0: 121, 1: 77, 2: 76, 3: 3, 4: 4, 5: 4, 6: 58, 7: 65, 8: 4, 9: 4}, 66: {0: 2, 1: 10, 2: 69, 3: 22, 4: 145, 5: 14, 6: 98, 7: 1, 8: 1, 9: 25}, 67: {0: 101, 1: 23, 3: 7, 4: 13, 5: 23, 6: 3, 7: 221, 8: 83, 9: 52}, 68: {0: 4, 1: 2, 2: 3, 3: 55, 5: 12, 6: 34, 7: 146, 8: 1, 9: 258}, 69: {0: 52, 1: 122, 2: 51, 3: 231, 4: 35, 6: 108}, 70: {0: 111, 1: 34, 2: 132, 3: 60, 4: 5, 5: 17, 6: 26, 7: 83, 8: 3, 9: 162}, 71: {0: 2, 1: 3, 2: 305, 3: 4, 4: 1, 5: 66, 6: 14, 7: 15, 8: 9, 9: 12}, 72: {0: 51, 1: 193, 3: 30, 4: 11, 5: 351}, 73: {0: 77, 1: 1, 2: 20, 3: 3, 4: 79, 5: 185, 6: 38, 7: 49, 8: 13, 9: 61}, 74: {0: 40, 1: 31, 2: 3, 3: 153, 4: 66, 5: 83, 7: 84, 8: 2, 9: 382}, 75: {0: 9, 3: 63, 4: 156, 5: 1, 6: 13, 7: 3, 8: 43, 9: 28}, 76: {0: 11, 1: 174, 2: 1, 3: 15, 5: 67, 6: 19, 7: 21, 8: 94, 9: 23}, 77: {0: 50, 2: 18, 4: 34, 5: 7, 6: 34, 7: 95, 8: 37, 9: 41}, 78: {1: 157, 2: 10, 3: 35, 4: 111, 6: 2, 7: 35, 8: 294}, 79: {0: 132, 1: 1, 2: 3, 3: 15, 5: 10, 6: 136, 7: 9, 8: 141, 9: 22}, 80: {0: 50, 1: 1, 2: 2, 3: 26, 4: 3, 5: 2, 6: 82, 7: 98, 8: 7, 9: 5}, 81: {0: 84, 1: 36, 2: 5, 3: 80, 4: 4, 5: 51, 6: 1, 7: 20, 8: 19, 9: 34}, 82: {0: 26, 1: 26, 2: 5, 5: 30, 6: 34, 7: 8, 8: 386}, 83: {0: 68, 1: 12, 2: 4, 3: 7, 4: 29, 5: 90, 6: 28, 7: 6, 8: 49, 9: 6}, 84: {0: 119, 1: 83, 3: 120, 4: 26, 5: 14, 6: 163}, 85: {0: 4, 1: 3, 2: 6, 3: 9, 4: 7, 5: 62, 7: 100, 8: 245, 9: 385}, 86: {0: 79, 1: 6, 2: 48, 3: 302, 4: 46, 5: 222}, 87: {0: 36, 1: 21, 2: 14, 3: 15, 4: 113, 5: 2, 6: 57, 7: 83, 8: 6, 9: 232}, 88: {0: 6, 1: 24, 2: 22, 4: 63, 5: 14, 6: 724}, 89: {0: 27, 1: 9, 2: 65, 3: 8, 5: 166, 6: 3, 7: 12, 8: 1}, 90: {0: 41, 1: 17, 2: 23, 3: 15, 4: 19, 5: 36, 6: 5, 7: 32, 8: 38, 9: 112}, 91: {0: 55, 2: 65, 3: 52, 5: 17, 6: 20, 7: 96, 9: 24}, 92: {0: 11, 1: 32, 2: 24, 3: 55, 4: 31, 5: 19, 6: 205, 7: 24, 8: 48, 9: 32}, 93: {0: 81, 1: 181, 2: 57, 3: 89, 4: 3, 5: 1, 6: 2, 7: 3, 8: 406}, 94: {0: 5, 1: 157, 2: 55, 3: 1, 4: 10, 5: 35, 6: 6, 7: 58, 8: 160, 9: 403}, 95: {0: 8, 1: 451, 2: 18, 3: 25}, 96: {0: 61, 1: 80, 4: 1, 5: 46, 6: 29, 7: 5, 8: 3, 9: 275}, 97: {0: 5, 1: 48, 2: 107, 3: 10, 4: 11, 5: 34, 6: 22, 7: 31, 8: 26, 9: 3}, 98: {0: 58, 1: 1, 3: 21, 4: 150, 5: 34, 6: 52, 7: 11, 8: 416}, 99: {0: 65, 1: 19, 2: 128, 3: 13, 4: 146, 5: 84, 6: 161, 7: 1, 9: 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Client ID:1,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:2,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:3,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:4,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:5,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:6,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:7,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:8,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:9,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:10,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:11,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:12,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:13,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:14,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:15,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:16,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:17,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:18,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:19,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:20,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:21,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:22,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:23,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:24,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:25,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:26,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:27,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:28,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:29,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:30,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:31,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:32,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:33,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:34,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:35,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:36,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:37,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:38,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:39,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:40,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:41,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:42,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:43,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:44,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:45,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:46,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:47,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:48,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:49,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:50,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:51,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:52,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:53,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:54,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:55,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:56,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:57,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:58,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:59,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:60,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:61,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:62,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:63,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:64,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:65,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:66,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:67,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:68,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:69,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:70,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:71,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:72,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:73,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:74,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:75,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:76,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:77,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:78,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:79,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:80,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:81,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:82,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:83,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:84,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:85,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:86,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:87,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:88,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:89,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:90,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:91,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:92,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:93,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:94,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:95,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:96,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:97,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:98,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:99,\tLocal Train Data Size:50000\n",
      "INFO:root:Client ID:100,\tLocal Train Data Size:50000\n"
     ]
    }
   ],
   "source": [
    "data_args = {\n",
    "'n_clients': 100,\n",
    "'partition' : 'noniid-labeldir',\n",
    "'dataset' : 'cifar10',\n",
    "'datadir' : './data',\n",
    "'beta' : .5,\n",
    "'train_bs' : 64,\n",
    "'test_bs' : 128,\n",
    "'n_worker' : 32\n",
    "}\n",
    "\n",
    "data_loaders, global_test_dl, _ = fedlib.init_dataset(**data_args)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model you want to deploy to FL\n",
    "Here I use the ResNet-20 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet20()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the FL algorithm\n",
    "Here I use the fedavg imported from fedlib, you could import other algorithms or you could also write your own algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = feddp(fedlib.get_logger())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create server and clients objects\n",
    "Here we use the arguments we defined before, and create server and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_args = {\n",
    "'n_clients' : 100,\n",
    "'global_model' : model,\n",
    "'device' : 'cpu', \n",
    "'sampler' : 'random',\n",
    "'test_dataset' : global_test_dl,\n",
    "'trainer' : trainer,\n",
    "'communicator' : None\n",
    "}\n",
    "server = Server(**server_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer: <fedlib.lib.algo.feddp.feddp.Trainer object at 0x7f9a1260dcf0>\n"
     ]
    }
   ],
   "source": [
    "client_args ={\n",
    "    'n_clients': 100,\n",
    "    'model': model,\n",
    "    'data_loaders':data_loaders,\n",
    "    'testloader':global_test_dl,\n",
    "    'trainer':trainer, \n",
    "    'lr':0.01,\n",
    "    'lr_scheduler':'ExponentialLR',\n",
    "    'criterion':'CrossEntropyLoss',\n",
    "    'optimizer':'sgd',\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "clients = fedlib.init_clients(**client_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simulator\n",
    "\n",
    "Simulator simulates the virtual federated learning environments, and run server and clients on single device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = simulator(server=server, clients=clients, \n",
    "                        communication_rounds=10,\n",
    "                        n_clients=100,\n",
    "                        participate_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulator\n",
    "User API Simulator.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*******starting Rounds 1 Optimization******\n",
      "INFO:root:Participate Clients: [ 2 61 89  5 70 39 31 34 27 90]\n",
      "INFO:root:Optimize the 2-th Clients\n",
      "INFO:root:Layer: conv1, Sparsity: 0.23%\n",
      "INFO:root:Layer: layer1.0.conv1, Sparsity: 0.78%\n",
      "INFO:root:Layer: layer1.0.conv2, Sparsity: 0.39%\n",
      "INFO:root:Layer: layer1.1.conv1, Sparsity: 0.69%\n",
      "INFO:root:Layer: layer1.1.conv2, Sparsity: 0.52%\n",
      "INFO:root:Layer: layer1.2.conv1, Sparsity: 0.82%\n",
      "INFO:root:Layer: layer1.2.conv2, Sparsity: 0.56%\n",
      "INFO:root:Layer: layer2.0.conv1, Sparsity: 0.93%\n",
      "INFO:root:Layer: layer2.0.conv2, Sparsity: 1.05%\n",
      "INFO:root:Layer: layer2.1.conv1, Sparsity: 0.95%\n",
      "INFO:root:Layer: layer2.1.conv2, Sparsity: 0.92%\n",
      "INFO:root:Layer: layer2.2.conv1, Sparsity: 0.97%\n",
      "INFO:root:Layer: layer2.2.conv2, Sparsity: 0.87%\n",
      "INFO:root:Layer: layer3.0.conv1, Sparsity: 0.79%\n",
      "INFO:root:Layer: layer3.0.conv2, Sparsity: 1.38%\n",
      "INFO:root:Layer: layer3.1.conv1, Sparsity: 1.30%\n",
      "INFO:root:Layer: layer3.1.conv2, Sparsity: 1.39%\n",
      "INFO:root:Layer: layer3.2.conv1, Sparsity: 1.37%\n",
      "INFO:root:Layer: layer3.2.conv2, Sparsity: 1.36%\n",
      "INFO:root:Model sparsity: 0.91%\n",
      "INFO:root:Epoch: 0\tLoss: 3.524756\tAccuracy:0.196181\tModel sparsity: 0.91%\n",
      "INFO:root:Layer: conv1, Sparsity: 0.23%\n",
      "INFO:root:Layer: layer1.0.conv1, Sparsity: 0.91%\n",
      "INFO:root:Layer: layer1.0.conv2, Sparsity: 0.48%\n",
      "INFO:root:Layer: layer1.1.conv1, Sparsity: 0.87%\n",
      "INFO:root:Layer: layer1.1.conv2, Sparsity: 0.65%\n",
      "INFO:root:Layer: layer1.2.conv1, Sparsity: 1.04%\n",
      "INFO:root:Layer: layer1.2.conv2, Sparsity: 0.61%\n",
      "INFO:root:Layer: layer2.0.conv1, Sparsity: 0.95%\n",
      "INFO:root:Layer: layer2.0.conv2, Sparsity: 1.13%\n",
      "INFO:root:Layer: layer2.1.conv1, Sparsity: 1.05%\n",
      "INFO:root:Layer: layer2.1.conv2, Sparsity: 1.00%\n",
      "INFO:root:Layer: layer2.2.conv1, Sparsity: 1.02%\n",
      "INFO:root:Layer: layer2.2.conv2, Sparsity: 0.94%\n",
      "INFO:root:Layer: layer3.0.conv1, Sparsity: 0.86%\n",
      "INFO:root:Layer: layer3.0.conv2, Sparsity: 1.47%\n",
      "INFO:root:Layer: layer3.1.conv1, Sparsity: 1.37%\n",
      "INFO:root:Layer: layer3.1.conv2, Sparsity: 1.49%\n",
      "INFO:root:Layer: layer3.2.conv1, Sparsity: 1.45%\n",
      "INFO:root:Layer: layer3.2.conv2, Sparsity: 1.43%\n",
      "INFO:root:Model sparsity: 1.00%\n",
      "INFO:root:Epoch: 1\tLoss: 2.928626\tAccuracy:0.348958\tModel sparsity: 1.00%\n",
      "INFO:root:Layer: conv1, Sparsity: 0.23%\n",
      "INFO:root:Layer: layer1.0.conv1, Sparsity: 1.04%\n",
      "INFO:root:Layer: layer1.0.conv2, Sparsity: 0.61%\n",
      "INFO:root:Layer: layer1.1.conv1, Sparsity: 1.00%\n",
      "INFO:root:Layer: layer1.1.conv2, Sparsity: 0.74%\n",
      "INFO:root:Layer: layer1.2.conv1, Sparsity: 1.09%\n",
      "INFO:root:Layer: layer1.2.conv2, Sparsity: 0.65%\n",
      "INFO:root:Layer: layer2.0.conv1, Sparsity: 0.98%\n",
      "INFO:root:Layer: layer2.0.conv2, Sparsity: 1.15%\n",
      "INFO:root:Layer: layer2.1.conv1, Sparsity: 1.11%\n",
      "INFO:root:Layer: layer2.1.conv2, Sparsity: 1.09%\n",
      "INFO:root:Layer: layer2.2.conv1, Sparsity: 1.11%\n",
      "INFO:root:Layer: layer2.2.conv2, Sparsity: 1.06%\n",
      "INFO:root:Layer: layer3.0.conv1, Sparsity: 0.88%\n",
      "INFO:root:Layer: layer3.0.conv2, Sparsity: 1.53%\n",
      "INFO:root:Layer: layer3.1.conv1, Sparsity: 1.42%\n",
      "INFO:root:Layer: layer3.1.conv2, Sparsity: 1.56%\n",
      "INFO:root:Layer: layer3.2.conv1, Sparsity: 1.51%\n",
      "INFO:root:Layer: layer3.2.conv2, Sparsity: 1.49%\n",
      "INFO:root:Model sparsity: 1.06%\n",
      "INFO:root:Epoch: 2\tLoss: 2.575788\tAccuracy:0.368056\tModel sparsity: 1.06%\n",
      "INFO:root:Layer: conv1, Sparsity: 0.23%\n",
      "INFO:root:Layer: layer1.0.conv1, Sparsity: 1.04%\n",
      "INFO:root:Layer: layer1.0.conv2, Sparsity: 0.69%\n",
      "INFO:root:Layer: layer1.1.conv1, Sparsity: 1.00%\n",
      "INFO:root:Layer: layer1.1.conv2, Sparsity: 0.82%\n",
      "INFO:root:Layer: layer1.2.conv1, Sparsity: 1.09%\n",
      "INFO:root:Layer: layer1.2.conv2, Sparsity: 0.65%\n",
      "INFO:root:Layer: layer2.0.conv1, Sparsity: 1.00%\n",
      "INFO:root:Layer: layer2.0.conv2, Sparsity: 1.24%\n",
      "INFO:root:Layer: layer2.1.conv1, Sparsity: 1.14%\n",
      "INFO:root:Layer: layer2.1.conv2, Sparsity: 1.13%\n",
      "INFO:root:Layer: layer2.2.conv1, Sparsity: 1.16%\n",
      "INFO:root:Layer: layer2.2.conv2, Sparsity: 1.14%\n",
      "INFO:root:Layer: layer3.0.conv1, Sparsity: 0.92%\n",
      "INFO:root:Layer: layer3.0.conv2, Sparsity: 1.59%\n",
      "INFO:root:Layer: layer3.1.conv1, Sparsity: 1.45%\n",
      "INFO:root:Layer: layer3.1.conv2, Sparsity: 1.62%\n",
      "INFO:root:Layer: layer3.2.conv1, Sparsity: 1.56%\n",
      "INFO:root:Layer: layer3.2.conv2, Sparsity: 1.55%\n",
      "INFO:root:Model sparsity: 1.11%\n",
      "INFO:root:Epoch: 3\tLoss: 2.338394\tAccuracy:0.394097\tModel sparsity: 1.11%\n"
     ]
    }
   ],
   "source": [
    "simulator.run(local_epochs=10,pruning_threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14599062b2f9b2c02bf607b744cd02923df131fc806bc02ca9049e6dcc66a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
